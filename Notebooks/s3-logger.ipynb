{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb1dcb80",
   "metadata": {},
   "source": [
    "### **1. Upload 10 parquets**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2ef905",
   "metadata": {},
   "source": [
    "#### Flow the following steps:\n",
    "1. Install pkgs\n",
    "pip install pandas pyarrow boto3 python-dotenv\n",
    "2. .env\n",
    "set all the credentials in that file\n",
    "3. Create and upload parquets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a288156",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Load modules --#\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aec15336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading 2024-01-01...\n"
     ]
    },
    {
     "ename": "S3UploadFailedError",
     "evalue": "Failed to upload forecast_2024-01-01.parquet to cdh-hydrolongterm-514438/longterm-forecast/monthly/run_date=2024-01-01/version=v1.0/ensemble/forecast.parquet: An error occurred (ExpiredToken) when calling the PutObject operation: The provided token has expired.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/dash/lib/python3.10/site-packages/boto3/s3/transfer.py:373\u001b[0m, in \u001b[0;36mS3Transfer.upload_file\u001b[0;34m(self, filename, bucket, key, callback, extra_args)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 373\u001b[0m     \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;66;03m# If a client error was raised, add the backwards compatibility layer\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;66;03m# that raises a S3UploadFailedError. These specific errors were only\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# ever thrown for upload_parts but now can be thrown for any related\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;66;03m# client error.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dash/lib/python3.10/site-packages/s3transfer/futures.py:111\u001b[0m, in \u001b[0;36mTransferFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# Usually the result() method blocks until the transfer is done,\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;66;03m# however if a KeyboardInterrupt is raised we want want to exit\u001b[39;00m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;66;03m# out of this and propagate the exception.\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_coordinator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/dash/lib/python3.10/site-packages/s3transfer/futures.py:287\u001b[0m, in \u001b[0;36mTransferCoordinator.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m--> 287\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m~/anaconda3/envs/dash/lib/python3.10/site-packages/s3transfer/tasks.py:142\u001b[0m, in \u001b[0;36mTask.__call__\u001b[0;34m(self, ctx)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_coordinator\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m--> 142\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/dash/lib/python3.10/site-packages/s3transfer/tasks.py:165\u001b[0m, in \u001b[0;36mTask._execute_main\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecuting task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with kwargs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs_to_display\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 165\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_main\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# If the task is the final task, then set the TransferFuture's\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# value to the return value from main().\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dash/lib/python3.10/site-packages/s3transfer/upload.py:796\u001b[0m, in \u001b[0;36mPutObjectTask._main\u001b[0;34m(self, client, fileobj, bucket, key, extra_args)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fileobj \u001b[38;5;28;01mas\u001b[39;00m body:\n\u001b[0;32m--> 796\u001b[0m     \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mput_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBucket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbucket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dash/lib/python3.10/site-packages/botocore/client.py:602\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dash/lib/python3.10/site-packages/botocore/context.py:123\u001b[0m, in \u001b[0;36mwith_current_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m     hook()\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dash/lib/python3.10/site-packages/botocore/client.py:1078\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1077\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1078\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ExpiredToken) when calling the PutObject operation: The provided token has expired.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mS3UploadFailedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m initDate \u001b[38;5;129;01min\u001b[39;00m rngDateIndex:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUploading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minitDate\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m%Y-%m-%d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 43\u001b[0m     \u001b[43ms3_upload_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitDate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUploaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minitDate\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m%Y-%m-%d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 35\u001b[0m, in \u001b[0;36ms3_upload_parquet\u001b[0;34m(initDate)\u001b[0m\n\u001b[1;32m     33\u001b[0m s3 \u001b[38;5;241m=\u001b[39m boto3\u001b[38;5;241m.\u001b[39mclient(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     34\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/monthly/run_date=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minitDate\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/version=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mversion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/ensemble/forecast.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 35\u001b[0m \u001b[43ms3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbucket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dash/lib/python3.10/site-packages/botocore/context.py:123\u001b[0m, in \u001b[0;36mwith_current_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook:\n\u001b[1;32m    122\u001b[0m     hook()\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dash/lib/python3.10/site-packages/boto3/s3/inject.py:175\u001b[0m, in \u001b[0;36mupload_file\u001b[0;34m(self, Filename, Bucket, Key, ExtraArgs, Callback, Config)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Upload a file to an S3 object.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03mUsage::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03m    transfer.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m S3Transfer(\u001b[38;5;28mself\u001b[39m, Config) \u001b[38;5;28;01mas\u001b[39;00m transfer:\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtransfer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbucket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBucket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mKey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mExtraArgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dash/lib/python3.10/site-packages/boto3/s3/transfer.py:379\u001b[0m, in \u001b[0;36mS3Transfer.upload_file\u001b[0;34m(self, filename, bucket, key, callback, extra_args)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;66;03m# If a client error was raised, add the backwards compatibility layer\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;66;03m# that raises a S3UploadFailedError. These specific errors were only\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# ever thrown for upload_parts but now can be thrown for any related\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;66;03m# client error.\u001b[39;00m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 379\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m S3UploadFailedError(\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to upload \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    381\u001b[0m             filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([bucket, key]), e\n\u001b[1;32m    382\u001b[0m         )\n\u001b[1;32m    383\u001b[0m     )\n",
      "\u001b[0;31mS3UploadFailedError\u001b[0m: Failed to upload forecast_2024-01-01.parquet to cdh-hydrolongterm-514438/longterm-forecast/monthly/run_date=2024-01-01/version=v1.0/ensemble/forecast.parquet: An error occurred (ExpiredToken) when calling the PutObject operation: The provided token has expired."
     ]
    }
   ],
   "source": [
    "#-- Create parquets --#\n",
    "def s3_upload_parquet(initDate: str):\n",
    "    #-- Create dataframe --#\n",
    "    rngDate = pd.date_range(start=initDate.strftime('%Y-%m-%d'), periods=18, freq='1MS')\n",
    "    nmember = 200\n",
    "    file_name = f\"Hydro-LongFc_{initDate:%Y-%m-%d}.parquet\"\n",
    "    basinName = ['QN-Mantaro', 'QN-Santa', 'QN-Rimac', 'QN-Vilcanota']\n",
    "    initDates = [initDate]*len(rngDate)*nmember*len(basinName)\n",
    "    #-- Dataframe --#\n",
    "    sampleData = pd.DataFrame(\n",
    "        {\n",
    "            'name': [s for s in basinName for _ in range(len(rngDate)) for member in range(nmember)],\n",
    "            'initDate': initDates,\n",
    "            'date': np.tile(rngDate.tolist(),len(basinName)*nmember),\n",
    "            'member': [i+1 for _ in range(len(rngDate)) for a in range(len(basinName)) for i in range(nmember)],\n",
    "            'QNfc': 100+5*np.random.randn((len(rngDate)),len(basinName),nmember).flatten(),\n",
    "            'QN': 100+5.5*np.random.randn((len(rngDate)),len(basinName),nmember).flatten(),\n",
    "            'QNclim': 90+5.3*np.random.randn((len(rngDate)),len(basinName),nmember).flatten(),\n",
    "        }\n",
    "    )\n",
    "    #-- Save locally --#\n",
    "    local_file = f'forecast_{initDate:%Y-%m-%d}.parquet'\n",
    "    sampleData.to_parquet(local_file, index=False)\n",
    "    #-- Set credentials --#\n",
    "    import boto3\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv('.env')\n",
    "    #-- Fix parameters --#\n",
    "    bucket = 'cdh-hydrolongterm-514438'\n",
    "    base = 'longterm-forecast'\n",
    "    version = 'v1.0'\n",
    "    #-- Upload to S3 --#\n",
    "    s3 = boto3.client('s3')\n",
    "    key = f\"{base}/monthly/run_date={initDate.strftime('%Y-%m-%d')}/version={version}/ensemble/forecast.parquet\"\n",
    "    s3.upload_file(local_file, bucket, key)\n",
    "\n",
    "#-- Run through dates --#\n",
    "initDate = '2024-01-01'\n",
    "endDate = '2024-12-01'\n",
    "rngDateIndex = pd.date_range(start=initDate, end=endDate, freq='1MS')\n",
    "for initDate in rngDateIndex:\n",
    "    print(f\"Uploading {initDate:%Y-%m-%d}...\")\n",
    "    s3_upload_parquet(initDate)\n",
    "    print(f\"Uploaded {initDate:%Y-%m-%d}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a04b4e",
   "metadata": {},
   "source": [
    "### **2. Query Buckets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fad5fa66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x7c9967c889b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-- Connect to DuckDB and set credentials --#\n",
    "import duckdb, os\n",
    "con = duckdb.connect()\n",
    "con.execute(\"SET s3_region=?\", [os.getenv(\"AWS_DEFAULT_REGION\")])\n",
    "con.execute(\"SET s3_access_key_id=?\", [os.getenv(\"AWS_ACCESS_KEY_ID\")])\n",
    "con.execute(\"SET s3_secret_access_key=?\", [os.getenv(\"AWS_SECRET_ACCESS_KEY\")])\n",
    "con.execute(\"SET s3_session_token=?\", [os.getenv(\"AWS_SESSION_TOKEN\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "579e9e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Build the list of dates to request --#\n",
    "bucket = 'cdh-hydrolongterm-514438'\n",
    "base = 'longterm-forecast'\n",
    "version = 'v1.0'\n",
    "uris = [\n",
    "    f\"s3://{bucket}/{base}/monthly/run_date={initDate.strftime('%Y-%m-%d')}/version={version}/ensemble/forecast.parquet\" \n",
    "    for initDate in rngDateIndex\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0315504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s3://cdh-hydrolongterm-514438/longterm-forecast/monthly/run_date=2024-01-01/version=v1.0/ensemble/forecast.parquet',\n",
       " 's3://cdh-hydrolongterm-514438/longterm-forecast/monthly/run_date=2024-02-01/version=v1.0/ensemble/forecast.parquet',\n",
       " 's3://cdh-hydrolongterm-514438/longterm-forecast/monthly/run_date=2024-03-01/version=v1.0/ensemble/forecast.parquet',\n",
       " 's3://cdh-hydrolongterm-514438/longterm-forecast/monthly/run_date=2024-04-01/version=v1.0/ensemble/forecast.parquet',\n",
       " 's3://cdh-hydrolongterm-514438/longterm-forecast/monthly/run_date=2024-05-01/version=v1.0/ensemble/forecast.parquet',\n",
       " 's3://cdh-hydrolongterm-514438/longterm-forecast/monthly/run_date=2024-06-01/version=v1.0/ensemble/forecast.parquet',\n",
       " 's3://cdh-hydrolongterm-514438/longterm-forecast/monthly/run_date=2024-07-01/version=v1.0/ensemble/forecast.parquet',\n",
       " 's3://cdh-hydrolongterm-514438/longterm-forecast/monthly/run_date=2024-08-01/version=v1.0/ensemble/forecast.parquet',\n",
       " 's3://cdh-hydrolongterm-514438/longterm-forecast/monthly/run_date=2024-09-01/version=v1.0/ensemble/forecast.parquet',\n",
       " 's3://cdh-hydrolongterm-514438/longterm-forecast/monthly/run_date=2024-10-01/version=v1.0/ensemble/forecast.parquet',\n",
       " 's3://cdh-hydrolongterm-514438/longterm-forecast/monthly/run_date=2024-11-01/version=v1.0/ensemble/forecast.parquet',\n",
       " 's3://cdh-hydrolongterm-514438/longterm-forecast/monthly/run_date=2024-12-01/version=v1.0/ensemble/forecast.parquet']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f2c91ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://cdh-hydrolongterm-514438/longterm-forecast/monthly/run_date=2024-01-01/version=v1.0/ensemble/forecast.parquet'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'s3://cdh-hydrolongterm-514438/longterm-forecast/monthly/run_date=2024-01-01/version=v1.0/ensemble/forecast.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87742310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x7c9967c889b0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-- Create a view merging all the parquets --#\n",
    "sql_union = \" UNION ALL \".join([f\"SELECT * FROM read_parquet('{u}')\" for u in uris])\n",
    "con.execute(f\"CREATE OR REPLACE VIEW raw_long AS {sql_union}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "967408b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     run_date   rows\n",
      "0  2024-01-01  14400\n",
      "1  2024-02-01  14400\n",
      "2  2024-03-01  14400\n",
      "3  2024-04-01  14400\n",
      "4  2024-05-01  14400\n",
      "5  2024-06-01  14400\n",
      "6  2024-07-01  14400\n",
      "7  2024-08-01  14400\n",
      "8  2024-09-01  14400\n",
      "9  2024-10-01  14400\n",
      "10 2024-11-01  14400\n",
      "11 2024-12-01  14400\n"
     ]
    }
   ],
   "source": [
    "#-- How many rows per run --#\n",
    "q1 = con.execute(\"\"\"\n",
    "    SELECT initDate AS run_date, COUNT(*) AS rows\n",
    "    FROM raw_long\n",
    "    GROUP BY 1 ORDER BY 1\n",
    "\"\"\").df()\n",
    "print(q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c08e3938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         name targetDate     QN_mean\n",
      "0  QN-Mantaro 2024-01-01   99.778802\n",
      "1  QN-Mantaro 2024-02-01   99.736616\n",
      "2  QN-Mantaro 2024-03-01  100.071649\n",
      "3  QN-Mantaro 2024-04-01   99.972756\n",
      "4  QN-Mantaro 2024-05-01  100.082035\n"
     ]
    }
   ],
   "source": [
    "#-- Mean of ensemble --#\n",
    "q2 = con.execute(\"\"\"\n",
    "    SELECT name, date AS targetDate, AVG(QNfc) AS QN_mean\n",
    "    FROM raw_long\n",
    "    GROUP BY name, targetDate\n",
    "    ORDER BY name, targetDate\n",
    "\"\"\").df()\n",
    "print(q2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1398340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         name targetDate       anom\n",
      "0  QN-Mantaro 2024-01-01   9.061734\n",
      "1  QN-Mantaro 2024-02-01  10.287259\n",
      "2  QN-Mantaro 2024-03-01  10.204871\n",
      "3  QN-Mantaro 2024-04-01  10.013447\n",
      "4  QN-Mantaro 2024-05-01   9.966547\n"
     ]
    }
   ],
   "source": [
    "#-- Anomalies --#\n",
    "q3 = con.execute(\"\"\"\n",
    "    SELECT name, date AS targetDate, AVG(QNfc) - AVG(QNclim) AS anom\n",
    "    FROM raw_long\n",
    "    GROUP BY name, targetDate\n",
    "    ORDER BY name, targetDate\n",
    "\"\"\").df()\n",
    "print(q3.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7a47b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>targetDate</th>\n",
       "      <th>anom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>QN-Mantaro</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>9.061734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>QN-Mantaro</td>\n",
       "      <td>2024-02-01</td>\n",
       "      <td>10.287259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>QN-Mantaro</td>\n",
       "      <td>2024-03-01</td>\n",
       "      <td>10.204871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>QN-Mantaro</td>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>10.013447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>QN-Mantaro</td>\n",
       "      <td>2024-05-01</td>\n",
       "      <td>9.966547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>QN-Vilcanota</td>\n",
       "      <td>2026-01-01</td>\n",
       "      <td>10.211200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>QN-Vilcanota</td>\n",
       "      <td>2026-02-01</td>\n",
       "      <td>10.261312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>QN-Vilcanota</td>\n",
       "      <td>2026-03-01</td>\n",
       "      <td>9.675485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>QN-Vilcanota</td>\n",
       "      <td>2026-04-01</td>\n",
       "      <td>10.130510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>QN-Vilcanota</td>\n",
       "      <td>2026-05-01</td>\n",
       "      <td>10.715551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             name targetDate       anom\n",
       "0      QN-Mantaro 2024-01-01   9.061734\n",
       "1      QN-Mantaro 2024-02-01  10.287259\n",
       "2      QN-Mantaro 2024-03-01  10.204871\n",
       "3      QN-Mantaro 2024-04-01  10.013447\n",
       "4      QN-Mantaro 2024-05-01   9.966547\n",
       "..            ...        ...        ...\n",
       "111  QN-Vilcanota 2026-01-01  10.211200\n",
       "112  QN-Vilcanota 2026-02-01  10.261312\n",
       "113  QN-Vilcanota 2026-03-01   9.675485\n",
       "114  QN-Vilcanota 2026-04-01  10.130510\n",
       "115  QN-Vilcanota 2026-05-01  10.715551\n",
       "\n",
       "[116 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b45a48b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28035/322717660.py:77: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_mean = pd.read_sql(sql_mean, conn)\n"
     ]
    },
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql: \nSELECT\n  name,\n  date AS targetDate,\n  AVG(QNfc) AS QN_mean\nFROM lake.hydro_longterm_raw\nWHERE version = 'v1.0'\n  AND run_date BETWEEN DATE '2024-01-01' AND DATE '2024-10-01'\nGROUP BY name, targetDate\nORDER BY name, targetDate\n\nCOLUMN_NOT_FOUND: line 8:16: Column 'targetdate' cannot be resolved or requester is not authorized to access requested resources\nunable to rollback",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/dash/lib/python3.10/site-packages/pandas/io/sql.py:2664\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[0;34m(self, sql, params)\u001b[0m\n\u001b[1;32m   2663\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2664\u001b[0m     \u001b[43mcur\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2665\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cur\n",
      "File \u001b[0;32m~/anaconda3/envs/dash/lib/python3.10/site-packages/pyathena/cursor.py:219\u001b[0m, in \u001b[0;36mCursor.execute\u001b[0;34m(self, operation, parameters, work_group, s3_staging_dir, cache_size, cache_expiration_time, result_reuse_enable, result_reuse_minutes, paramstyle, on_start_query_execution, **kwargs)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 219\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OperationalError(query_execution\u001b[38;5;241m.\u001b[39mstate_change_reason)\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mOperationalError\u001b[0m: COLUMN_NOT_FOUND: line 8:16: Column 'targetdate' cannot be resolved or requester is not authorized to access requested resources",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotSupportedError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/dash/lib/python3.10/site-packages/pandas/io/sql.py:2668\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[0;34m(self, sql, params)\u001b[0m\n\u001b[1;32m   2667\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2668\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2669\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m inner_exc:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dash/lib/python3.10/site-packages/pyathena/connection.py:586\u001b[0m, in \u001b[0;36mConnection.rollback\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Rollback any pending transaction.\u001b[39;00m\n\u001b[1;32m    579\u001b[0m \n\u001b[1;32m    580\u001b[0m \u001b[38;5;124;03mThis method is required by DB API 2.0 but is not supported by Athena\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[38;5;124;03m    NotSupportedError: Always raised since transactions are not supported.\u001b[39;00m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 586\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m NotSupportedError\n",
      "\u001b[0;31mNotSupportedError\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 77\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# 4) Consulta de prueba (mean del ensamble por cuenca y mes objetivo)\u001b[39;00m\n\u001b[1;32m     66\u001b[0m sql_mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124mSELECT\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124m  name,\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124mORDER BY name, targetDate\u001b[39m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 77\u001b[0m df_mean \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_mean\u001b[38;5;241m.\u001b[39mhead())\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# 5) Crear CURATED con CTAS (Parquet + particionado + tu ubicación)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dash/lib/python3.10/site-packages/pandas/io/sql.py:708\u001b[0m, in \u001b[0;36mread_sql\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize, dtype_backend, dtype)\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pandas_sql, SQLiteDatabase):\n\u001b[0;32m--> 708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m            \u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    720\u001b[0m         _is_table_name \u001b[38;5;241m=\u001b[39m pandas_sql\u001b[38;5;241m.\u001b[39mhas_table(sql)\n",
      "File \u001b[0;32m~/anaconda3/envs/dash/lib/python3.10/site-packages/pandas/io/sql.py:2728\u001b[0m, in \u001b[0;36mSQLiteDatabase.read_query\u001b[0;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[1;32m   2717\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread_query\u001b[39m(\n\u001b[1;32m   2718\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2719\u001b[0m     sql,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2726\u001b[0m     dtype_backend: DtypeBackend \u001b[38;5;241m|\u001b[39m Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2727\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Iterator[DataFrame]:\n\u001b[0;32m-> 2728\u001b[0m     cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2729\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [col_desc[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m col_desc \u001b[38;5;129;01min\u001b[39;00m cursor\u001b[38;5;241m.\u001b[39mdescription]\n\u001b[1;32m   2731\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/dash/lib/python3.10/site-packages/pandas/io/sql.py:2673\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[0;34m(self, sql, params)\u001b[0m\n\u001b[1;32m   2669\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m inner_exc:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m   2670\u001b[0m     ex \u001b[38;5;241m=\u001b[39m DatabaseError(\n\u001b[1;32m   2671\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution failed on sql: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msql\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124munable to rollback\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2672\u001b[0m     )\n\u001b[0;32m-> 2673\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01minner_exc\u001b[39;00m\n\u001b[1;32m   2675\u001b[0m ex \u001b[38;5;241m=\u001b[39m DatabaseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution failed on sql \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msql\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2676\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mDatabaseError\u001b[0m: Execution failed on sql: \nSELECT\n  name,\n  date AS targetDate,\n  AVG(QNfc) AS QN_mean\nFROM lake.hydro_longterm_raw\nWHERE version = 'v1.0'\n  AND run_date BETWEEN DATE '2024-01-01' AND DATE '2024-10-01'\nGROUP BY name, targetDate\nORDER BY name, targetDate\n\nCOLUMN_NOT_FOUND: line 8:16: Column 'targetdate' cannot be resolved or requester is not authorized to access requested resources\nunable to rollback"
     ]
    }
   ],
   "source": [
    "# --- Parámetros de TU dataset / región ---\n",
    "DB            = \"lake\"\n",
    "REGION        = \"eu-west-1\"\n",
    "WORKGROUP     = \"primary\"  # cambia si usas otro\n",
    "BUCKET        = \"cdh-hydrolongterm-514438\"\n",
    "BASE          = \"longterm-forecasts\"\n",
    "VERSION       = \"v1.0\"\n",
    "\n",
    "# Athena necesita un staging S3 donde pueda escribir resultados de consulta:\n",
    "# Usa una carpeta del MISMO bucket si tienes permiso de escritura; si no, usa otro bucket propio.\n",
    "ATHENA_STAGING = f\"s3://{BUCKET}/athena-staging/\"   # ajústalo si te da AccessDenied\n",
    "\n",
    "# --- Conexión Athena ---\n",
    "from pyathena import connect\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "conn = connect(\n",
    "    s3_staging_dir=ATHENA_STAGING,\n",
    "    region_name=REGION,\n",
    "    work_group=WORKGROUP,\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# 1) DB (si no existe)\n",
    "cur.execute(f\"CREATE DATABASE IF NOT EXISTS {DB}\")\n",
    "\n",
    "# 2) Tabla externa sobre RAW (particionada por ruta)\n",
    "# Ubicación física:\n",
    "RAW_LOCATION = f\"s3://{BUCKET}/{BASE}/raw/\"\n",
    "\n",
    "cur.execute(f\"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS {DB}.hydro_longterm_raw (\n",
    "  name        string,\n",
    "  initDate    date,\n",
    "  date        date,\n",
    "  member      int,\n",
    "  QNfc        double,\n",
    "  QN          double,\n",
    "  QNclim      double\n",
    ")\n",
    "PARTITIONED BY (\n",
    "  run_date date,\n",
    "  version  string\n",
    ")\n",
    "STORED AS PARQUET\n",
    "LOCATION '{RAW_LOCATION}'\n",
    "\"\"\")\n",
    "\n",
    "# 3) Agregar particiones explícitas (enero..octubre 2024, v1.0)\n",
    "run_dates = [date(2024,1,1) + relativedelta(months=i) for i in range(10)]\n",
    "\n",
    "for rd in run_dates:\n",
    "    part_loc = (\n",
    "        f\"s3://{BUCKET}/{BASE}/raw/\"\n",
    "        f\"run_date={rd:%Y-%m-%d}/version={VERSION}/\"\n",
    "    )\n",
    "    cur.execute(f\"\"\"\n",
    "    ALTER TABLE {DB}.hydro_longterm_raw\n",
    "      ADD IF NOT EXISTS PARTITION (run_date=DATE '{rd:%Y-%m-%d}', version='{VERSION}')\n",
    "      LOCATION '{part_loc}'\n",
    "    \"\"\")\n",
    "\n",
    "# 4) Consulta de prueba (mean del ensamble por cuenca y mes objetivo)\n",
    "sql_mean = f\"\"\"\n",
    "SELECT\n",
    "  name,\n",
    "  date AS targetDate,\n",
    "  AVG(QNfc) AS QN_mean\n",
    "FROM {DB}.hydro_longterm_raw\n",
    "WHERE version = '{VERSION}'\n",
    "  AND run_date BETWEEN DATE '2024-01-01' AND DATE '2024-10-01'\n",
    "GROUP BY name, targetDate\n",
    "ORDER BY name, targetDate\n",
    "\"\"\"\n",
    "df_mean = pd.read_sql(sql_mean, conn)\n",
    "print(df_mean.head())\n",
    "\n",
    "# 5) Crear CURATED con CTAS (Parquet + particionado + tu ubicación)\n",
    "CURATED_MEAN_LOCATION = f\"s3://{BUCKET}/{BASE}/curated/meanEnsemble/\"\n",
    "\n",
    "# Si la tabla ya existe, puedes DROP TABLE primero o hacer INSERT en una ya creada\n",
    "cur.execute(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {DB}.longterm_mean\n",
    "WITH (\n",
    "  format = 'PARQUET',\n",
    "  external_location = '{CURATED_MEAN_LOCATION}',\n",
    "  partitioned_by = ARRAY['run_date','version']\n",
    ") AS\n",
    "SELECT\n",
    "  name,\n",
    "  date AS targetDate,\n",
    "  AVG(QNfc) AS QN_mean,\n",
    "  run_date,\n",
    "  version\n",
    "FROM {DB}.hydro_longterm_raw\n",
    "WHERE version = '{VERSION}'\n",
    "  AND run_date BETWEEN DATE '2024-01-01' AND DATE '2024-10-01'\n",
    "GROUP BY name, date, run_date, version\n",
    "\"\"\")\n",
    "\n",
    "# (Opcional) Leer el curated recién creado:\n",
    "df_curated = pd.read_sql(f\"SELECT * FROM {DB}.longterm_mean ORDER BY name, targetDate LIMIT 20\", conn)\n",
    "print(df_curated.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad6ef7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c17438",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b940dbe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d36d169",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5479b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de03c33c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19238b27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607ee8eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3://cdh-hydrolongterm-514438/longterm-forecast/monthly/run_date=2024-01-01/version=v1.0/ensemble/forecast.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ecb93410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>QN-Mantaro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>QN-Mantaro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>QN-Mantaro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>QN-Mantaro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>QN-Mantaro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>QN-Vilcanota</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>QN-Vilcanota</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>QN-Vilcanota</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>QN-Vilcanota</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>QN-Vilcanota</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>360 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             name\n",
       "0      QN-Mantaro\n",
       "1      QN-Mantaro\n",
       "2      QN-Mantaro\n",
       "3      QN-Mantaro\n",
       "4      QN-Mantaro\n",
       "..            ...\n",
       "355  QN-Vilcanota\n",
       "356  QN-Vilcanota\n",
       "357  QN-Vilcanota\n",
       "358  QN-Vilcanota\n",
       "359  QN-Vilcanota\n",
       "\n",
       "[360 rows x 1 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_upload_parquet(datetime.datetime(2024, 1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72b4087",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8677b305",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randn(18, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8ed718",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.randn(3, 12)\n",
    "merged_array = data.flatten()[:26]\n",
    "print(merged_array)\n",
    "print(merged_array.shape)  # (26,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "032bb80a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.randn(3, 12).flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e0f14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InitDate_2024-01-01\n",
      "InitDate_2024-01-02\n",
      "InitDate_2024-01-03\n",
      "InitDate_2024-01-04\n",
      "InitDate_2024-01-05\n",
      "InitDate_2024-01-06\n",
      "InitDate_2024-01-07\n",
      "InitDate_2024-01-08\n",
      "InitDate_2024-01-09\n",
      "InitDate_2024-01-10\n"
     ]
    }
   ],
   "source": [
    "rng_date = pd.date_range(start=\"2024-01-01\", end=\"2024-01-10\", freq='D')\n",
    "for single_date in rng_date:\n",
    "    # print(single_date.strftime(\"%Y-%m-%d\"))\n",
    "    print(f\"InitDate_{single_date:%Y-%m-%d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26218a5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5e0dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7064cf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://cdh-hydrolongterm-514438.s3.eu-west-1.amazonaws.com/longterm-forecasts/raw/20250911_160434_input.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c05fecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90f338f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'R8YQRC84SW39NQ3M',\n",
       "  'HostId': 'UThAGF2Jx5y8+R+pH9VWk3pZqslIy7xgpT6uTpINUXeYQ3lvwysyWjgM/WphbbFBudvOf1BVwLE=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'UThAGF2Jx5y8+R+pH9VWk3pZqslIy7xgpT6uTpINUXeYQ3lvwysyWjgM/WphbbFBudvOf1BVwLE=',\n",
       "   'x-amz-request-id': 'R8YQRC84SW39NQ3M',\n",
       "   'date': 'Thu, 11 Sep 2025 16:35:32 GMT',\n",
       "   'last-modified': 'Thu, 11 Sep 2025 16:04:38 GMT',\n",
       "   'etag': '\"54054e8ae83393e08e2503d0244b30aa\"',\n",
       "   'x-amz-server-side-encryption': 'aws:kms',\n",
       "   'x-amz-server-side-encryption-aws-kms-key-id': 'arn:aws:kms:eu-west-1:228119973315:key/95a16e40-70e3-4520-baba-7a90da7bcc5a',\n",
       "   'x-amz-server-side-encryption-bucket-key-enabled': 'true',\n",
       "   'accept-ranges': 'bytes',\n",
       "   'content-type': 'binary/octet-stream',\n",
       "   'content-length': '18687',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 1},\n",
       " 'AcceptRanges': 'bytes',\n",
       " 'LastModified': datetime.datetime(2025, 9, 11, 16, 4, 38, tzinfo=tzutc()),\n",
       " 'ContentLength': 18687,\n",
       " 'ETag': '\"54054e8ae83393e08e2503d0244b30aa\"',\n",
       " 'ContentType': 'binary/octet-stream',\n",
       " 'ServerSideEncryption': 'aws:kms',\n",
       " 'Metadata': {},\n",
       " 'SSEKMSKeyId': 'arn:aws:kms:eu-west-1:228119973315:key/95a16e40-70e3-4520-baba-7a90da7bcc5a',\n",
       " 'BucketKeyEnabled': True}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "s3 = boto3.client(\"s3\")\n",
    "s3.head_object(\n",
    "    Bucket=\"cdh-hydrolongterm-514438\",\n",
    "    Key=\"longterm-forecasts/raw/20250911_160434_input.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "752eaa1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>NombreEmpresa</th>\n",
       "      <th>Tipoinfoabrev</th>\n",
       "      <th>power</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-09-05 00:30:00</td>\n",
       "      <td>ENGIE</td>\n",
       "      <td>MW</td>\n",
       "      <td>107.0830</td>\n",
       "      <td>W.F. Punta Lomitas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-09-05 01:00:00</td>\n",
       "      <td>ENGIE</td>\n",
       "      <td>MW</td>\n",
       "      <td>107.0830</td>\n",
       "      <td>W.F. Punta Lomitas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-09-05 01:30:00</td>\n",
       "      <td>ENGIE</td>\n",
       "      <td>MW</td>\n",
       "      <td>102.6775</td>\n",
       "      <td>W.F. Punta Lomitas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-09-05 02:00:00</td>\n",
       "      <td>ENGIE</td>\n",
       "      <td>MW</td>\n",
       "      <td>102.6775</td>\n",
       "      <td>W.F. Punta Lomitas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-09-05 02:30:00</td>\n",
       "      <td>ENGIE</td>\n",
       "      <td>MW</td>\n",
       "      <td>99.3825</td>\n",
       "      <td>W.F. Punta Lomitas</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date NombreEmpresa Tipoinfoabrev     power  \\\n",
       "0 2025-09-05 00:30:00         ENGIE            MW  107.0830   \n",
       "1 2025-09-05 01:00:00         ENGIE            MW  107.0830   \n",
       "2 2025-09-05 01:30:00         ENGIE            MW  102.6775   \n",
       "3 2025-09-05 02:00:00         ENGIE            MW  102.6775   \n",
       "4 2025-09-05 02:30:00         ENGIE            MW   99.3825   \n",
       "\n",
       "                 name  \n",
       "0  W.F. Punta Lomitas  \n",
       "1  W.F. Punta Lomitas  \n",
       "2  W.F. Punta Lomitas  \n",
       "3  W.F. Punta Lomitas  \n",
       "4  W.F. Punta Lomitas  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "uri = \"s3://cdh-hydrolongterm-514438/longterm-forecasts/raw/20250911_160434_input.parquet\"\n",
    "df = pd.read_parquet(uri)   # con s3fs/pyarrow y tus env vars, debería funcionar\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14099e49",
   "metadata": {},
   "source": [
    "### **Subir parquet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d16bef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo subido a: s3://cdh-hydrolongterm-514438/longterm-forecasts/raw/test_forecast.parquet\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "# Ruta exacta en tu bucket (ajusta run_date y version si quieres)\n",
    "bucket = \"cdh-hydrolongterm-514438\"\n",
    "key = \"longterm-forecasts/raw/test_forecast.parquet\"\n",
    "\n",
    "# Subir archivo local al S3/CDH\n",
    "s3.upload_file(\"../dataset/currentGen.parquet\", bucket, key)\n",
    "\n",
    "print(\"Archivo subido a:\", f\"s3://{bucket}/{key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dcf3a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    date       NombreEmpresa Tipoinfoabrev     power  \\\n",
      "0    2025-09-05 00:30:00               ENGIE            MW  107.0830   \n",
      "1    2025-09-05 01:00:00               ENGIE            MW  107.0830   \n",
      "2    2025-09-05 01:30:00               ENGIE            MW  102.6775   \n",
      "3    2025-09-05 02:00:00               ENGIE            MW  102.6775   \n",
      "4    2025-09-05 02:30:00               ENGIE            MW   99.3825   \n",
      "...                  ...                 ...           ...       ...   \n",
      "2875 2025-09-10 22:00:00  ORYGEN PERU S.A.A.            MW  115.1340   \n",
      "2876 2025-09-10 22:30:00  ORYGEN PERU S.A.A.            MW  114.4970   \n",
      "2877 2025-09-10 23:00:00  ORYGEN PERU S.A.A.            MW  114.4970   \n",
      "2878 2025-09-10 23:30:00  ORYGEN PERU S.A.A.            MW  112.3075   \n",
      "2879 2025-09-11 00:00:00  ORYGEN PERU S.A.A.            MW  112.3075   \n",
      "\n",
      "                    name  \n",
      "0     W.F. Punta Lomitas  \n",
      "1     W.F. Punta Lomitas  \n",
      "2     W.F. Punta Lomitas  \n",
      "3     W.F. Punta Lomitas  \n",
      "4     W.F. Punta Lomitas  \n",
      "...                  ...  \n",
      "2875        W.F. Wayra I  \n",
      "2876        W.F. Wayra I  \n",
      "2877        W.F. Wayra I  \n",
      "2878        W.F. Wayra I  \n",
      "2879        W.F. Wayra I  \n",
      "\n",
      "[2880 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Leer directo desde S3 (usando pandas + s3fs)\n",
    "uri = f\"s3://{bucket}/{key}\"\n",
    "df_check = pd.read_parquet(uri)\n",
    "print(df_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d39d51ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/carlosenciso/Documents/ENGIE/windShortTermForecast/Notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd92815",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d1a532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Cargar variables del archivo .env\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "# Verificación rápida\n",
    "print(\"Access Key:\", os.getenv(\"AWS_ACCESS_KEY_ID\")[:5], \"...\")\n",
    "print(\"Region:\", os.getenv(\"AWS_DEFAULT_REGION\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf2ee36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef1b5fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e738f9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c71a1c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f49c430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "resp = s3.list_objects_v2(\n",
    "    Bucket=\"cdh-dsdatalakecoesprod-514438\",\n",
    "    Prefix=\"projects/hydroForecast-Peru/datasources/Hydro-LongTerm/datasets/LongTerm-Forecasts/raw/\"\n",
    ")\n",
    "for obj in resp.get(\"Contents\", []):\n",
    "    print(obj[\"Key\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e752a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b6fcc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98296931",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9241c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Send to CDH --#\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d391d099",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Load dotenv --#\n",
    "load_dotenv()\n",
    "#-- Parquet file --#\n",
    "parquetFile = '../dataset/currentGen.parquet'\n",
    "print(f'File to send: {parquetFile}')\n",
    "#-- The file exits --#\n",
    "if os.path.exists(parquetFile):\n",
    "    size_mb = os.path.getsize(parquetFile) / (1024 * 1024)\n",
    "    print(f'File size: {size_mb:.2f} MB')\n",
    "else:\n",
    "    print(f'File {parquetFile} does not exist.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24efebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Credentials --#\n",
    "aws_access_key = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "aws_secret_key = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "aws_token = os.getenv('AWS_SESSION_TOKEN')\n",
    "aws_region = os.getenv('AWS_REGION', 'us-east-1')\n",
    "bucket_name = os.getenv('BUCKET_NAME', 'hydroforecast-peru-data')\n",
    "print(f\"🔑 Usando región: {aws_region}\")\n",
    "print(f\"🪣 Bucket destino: {bucket_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb01c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=aws_access_key,\n",
    "    aws_secret_access_key=aws_secret_key,\n",
    "    aws_session_token=aws_token,\n",
    "    region_name=aws_region\n",
    ")\n",
    "print(\"✅ Cliente S3 creado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2548c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.basename(parquetFile) \n",
    "s3_key = f\"hydroForecast-Peru/data/{filename}\"\n",
    "print(f\"📍 Se guardará en: s3://{bucket_name}/{s3_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090950f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"📤 Subiendo archivo...\")\n",
    "    s3_client.upload_file(\n",
    "        parquetFile,    \n",
    "        bucket_name,    \n",
    "        s3_key          \n",
    "    )\n",
    "    print(\"🎉 ¡SUBIDA EXITOSA!\")\n",
    "    print(f\"🌐 Tu archivo está en: s3://{bucket_name}/{s3_key}\")\n",
    "except Exception as error:\n",
    "    print(f\"❌ Error: {str(error)}\")\n",
    "print(\"🏁 Proceso terminado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c621e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Send to CDH --#\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "#-- Load dotenv --#\n",
    "load_dotenv()\n",
    "#-- Parquet file --#\n",
    "parquetFile = '../dataset/currentGen.parquet'\n",
    "print(f'File to send: {parquetFile}')\n",
    "#-- The file exits --#\n",
    "if os.path.exists(parquetFile):\n",
    "    size_mb = os.path.getsize(parquetFile) / (1024 * 1024)\n",
    "    print(f'File size: {size_mb:.2f} MB')\n",
    "else:\n",
    "    print(f'File {parquetFile} does not exist.')\n",
    "    exit()\n",
    "\n",
    "#-- Credentials --#\n",
    "aws_access_key = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "aws_secret_key = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "aws_token = os.getenv('AWS_SESSION_TOKEN')\n",
    "aws_region = os.getenv('AWS_REGION', 'us-east-1')\n",
    "bucket_name = os.getenv('BUCKET_NAME', 'hydroforecast-peru-data')\n",
    "print(f\"🔑 Usando región: {aws_region}\")\n",
    "print(f\"🪣 Bucket destino: {bucket_name}\")\n",
    "\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=aws_access_key,\n",
    "    aws_secret_access_key=aws_secret_key,\n",
    "    aws_session_token=aws_token,\n",
    "    region_name=aws_region\n",
    ")\n",
    "print(\"✅ Cliente S3 creado\")\n",
    "\n",
    "# Función para verificar y crear el bucket si no existe\n",
    "def check_and_create_bucket(bucket_name, region):\n",
    "    try:\n",
    "        # Verificar si el bucket existe\n",
    "        s3_client.head_bucket(Bucket=bucket_name)\n",
    "        print(f\"✅ Bucket '{bucket_name}' existe\")\n",
    "        return True\n",
    "    except s3_client.exceptions.ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        if error_code == '404':\n",
    "            print(f\"❌ Bucket '{bucket_name}' no existe, intentando crearlo...\")\n",
    "            try:\n",
    "                if region == 'us-east-1':\n",
    "                    # us-east-1 tiene una sintaxis especial\n",
    "                    s3_client.create_bucket(Bucket=bucket_name)\n",
    "                else:\n",
    "                    s3_client.create_bucket(\n",
    "                        Bucket=bucket_name,\n",
    "                        CreateBucketConfiguration={'LocationConstraint': region}\n",
    "                    )\n",
    "                print(f\"✅ Bucket '{bucket_name}' creado exitosamente\")\n",
    "                return True\n",
    "            except Exception as create_error:\n",
    "                print(f\"❌ Error creando bucket: {create_error}\")\n",
    "                return False\n",
    "        else:\n",
    "            print(f\"❌ Error accediendo al bucket: {e}\")\n",
    "            return False\n",
    "\n",
    "# Verificar y crear el bucket si es necesario\n",
    "if not check_and_create_bucket(bucket_name, aws_region):\n",
    "    print(\"No se pudo acceder al bucket, terminando ejecución.\")\n",
    "    exit()\n",
    "\n",
    "filename = os.path.basename(parquetFile) \n",
    "s3_key = f\"hydroForecast-Peru/data/{filename}\"\n",
    "print(f\"📍 Se guardará en: s3://{bucket_name}/{s3_key}\")\n",
    "\n",
    "try:\n",
    "    print(\"📤 Subiendo archivo...\")\n",
    "    s3_client.upload_file(\n",
    "        parquetFile,    \n",
    "        bucket_name,    \n",
    "        s3_key          \n",
    "    )\n",
    "    print(\"🎉 ¡SUBIDA EXITOSA!\")\n",
    "    print(f\"🌐 Tu archivo está en: s3://{bucket_name}/{s3_key}\")\n",
    "except Exception as error:\n",
    "    print(f\"❌ Error: {str(error)}\")\n",
    "print(\"🏁 Proceso terminado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78545fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "arn:aws:iam::228119973315:role/cdh_hydroforecastperu_78495"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe3c627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27afaef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Read dataset from Athena S3 --#\n",
    "import boto3\n",
    "import pandas as pd\n",
    "#-- Main code --#\n",
    "s3 = boto3.client('s3')\n",
    "\"\"\"\n",
    "bucket = 'your-bucket-name'\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b72f698",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78702464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf84fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize S3 client (requires AWS credentials configured in ~/.aws/credentials or env vars)\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "bucket = \"cdh-dsdatalakecoesprod-514438\"\n",
    "prefix = \"central_generadora/\"\n",
    "\n",
    "# List objects\n",
    "response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "for obj in response.get('Contents', []):\n",
    "    print(obj['Key'])\n",
    "\n",
    "# Read one file (example CSV/JSON/Parquet)\n",
    "obj = s3.get_object(Bucket=bucket, Key=\"central_generadora/example.csv\")\n",
    "data = obj['Body'].read().decode('utf-8')\n",
    "\n",
    "print(data[:500])  # preview first 500 chars\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dash",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
